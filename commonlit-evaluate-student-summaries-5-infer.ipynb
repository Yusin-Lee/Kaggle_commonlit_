{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06aa3730",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-10-11T09:00:53.029872Z",
     "iopub.status.busy": "2023-10-11T09:00:53.029108Z",
     "iopub.status.idle": "2023-10-11T09:04:09.141071Z",
     "shell.execute_reply": "2023-10-11T09:04:09.139948Z"
    },
    "papermill": {
     "duration": 196.121613,
     "end_time": "2023-10-11T09:04:09.143281",
     "exception": false,
     "start_time": "2023-10-11T09:00:53.021668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/bitsandbytes/bitsandbytes-0.41.1-py3-none-any.whl\r\n",
      "Installing collected packages: bitsandbytes\r\n",
      "Successfully installed bitsandbytes-0.41.1\r\n",
      "Processing /kaggle/input/peft-whl/peft-0.4.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (1.23.5)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (21.3)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (5.9.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (6.0)\r\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (2.0.0)\r\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (4.30.2)\r\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (0.20.3)\r\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (0.3.1)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.4.0) (3.0.9)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (3.12.2)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (4.6.3)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (3.1.2)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.4.0) (0.16.4)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.4.0) (2023.6.3)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.4.0) (2.31.0)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.4.0) (0.13.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.4.0) (4.65.0)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->peft==0.4.0) (2023.6.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.4.0) (2.1.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.4.0) (3.1.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.4.0) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.4.0) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.4.0) (2023.5.7)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.4.0) (1.3.0)\r\n",
      "Installing collected packages: peft\r\n",
      "Successfully installed peft-0.4.0\r\n",
      "Processing /kaggle/input/textstat-pypi/Pyphen-0.9.3-py2.py3-none-any.whl\r\n",
      "Installing collected packages: Pyphen\r\n",
      "Successfully installed Pyphen-0.9.3\r\n",
      "Processing /kaggle/input/textstat-pypi/textstat-0.7.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: pyphen in /opt/conda/lib/python3.10/site-packages (from textstat==0.7.0) (0.9.3)\r\n",
      "Installing collected packages: textstat\r\n",
      "Successfully installed textstat-0.7.0\r\n",
      "Processing /kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\r\n",
      "Installing collected packages: pyspellchecker\r\n",
      "Successfully installed pyspellchecker-0.7.2\r\n",
      "Processing /kaggle/input/autocorrect/autocorrect-2.6.1.tar\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hBuilding wheels for collected packages: autocorrect\r\n",
      "  Building wheel for autocorrect (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622383 sha256=8afabefb5bd7b092e1c1420f95eee3afc248841131050754fd919ea9471c8b09\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/db/69/42/0fb0421d2fe70d195a04665edc760cfe5fd341d7bb8d8e0aaa\r\n",
      "Successfully built autocorrect\r\n",
      "Installing collected packages: autocorrect\r\n",
      "Successfully installed autocorrect-2.6.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/bitsandbytes/bitsandbytes-0.41.1-py3-none-any.whl\n",
    "!pip install /kaggle/input/peft-whl/peft-0.4.0-py3-none-any.whl\n",
    "!pip install /kaggle/input/textstat-pypi/Pyphen-0.9.3-py2.py3-none-any.whl\n",
    "!pip install /kaggle/input/textstat-pypi/textstat-0.7.0-py3-none-any.whl\n",
    "!pip install /kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\n",
    "!pip install /kaggle/input/autocorrect/autocorrect-2.6.1.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d86afe4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T09:04:09.158079Z",
     "iopub.status.busy": "2023-10-11T09:04:09.157795Z",
     "iopub.status.idle": "2023-10-11T09:04:26.914042Z",
     "shell.execute_reply": "2023-10-11T09:04:26.913143Z"
    },
    "papermill": {
     "duration": 17.766013,
     "end_time": "2023-10-11T09:04:26.916181",
     "exception": false,
     "start_time": "2023-10-11T09:04:09.150168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import textstat\n",
    "import catboost\n",
    "import lightgbm\n",
    "import spacy\n",
    "import nltk\n",
    "from autocorrect import Speller\n",
    "from spellchecker import SpellChecker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from tqdm import tqdm, trange\n",
    "import bitsandbytes\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n",
    "import os\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "768800ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T09:04:26.931051Z",
     "iopub.status.busy": "2023-10-11T09:04:26.930357Z",
     "iopub.status.idle": "2023-10-11T09:04:26.937164Z",
     "shell.execute_reply": "2023-10-11T09:04:26.936369Z"
    },
    "papermill": {
     "duration": 0.015907,
     "end_time": "2023-10-11T09:04:26.938874",
     "exception": false,
     "start_time": "2023-10-11T09:04:26.922967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "200b376c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T09:04:26.952648Z",
     "iopub.status.busy": "2023-10-11T09:04:26.952377Z",
     "iopub.status.idle": "2023-10-11T09:04:26.957158Z",
     "shell.execute_reply": "2023-10-11T09:04:26.956338Z"
    },
    "papermill": {
     "duration": 0.01349,
     "end_time": "2023-10-11T09:04:26.958808",
     "exception": false,
     "start_time": "2023-10-11T09:04:26.945318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIR = './'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59993a9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T09:04:26.972799Z",
     "iopub.status.busy": "2023-10-11T09:04:26.972300Z",
     "iopub.status.idle": "2023-10-11T09:04:26.976497Z",
     "shell.execute_reply": "2023-10-11T09:04:26.975634Z"
    },
    "papermill": {
     "duration": 0.012924,
     "end_time": "2023-10-11T09:04:26.978164",
     "exception": false,
     "start_time": "2023-10-11T09:04:26.965240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    select = 'large'\n",
    "    model_name = f'/kaggle/input/deberta-v3-{select}-hf-weights'\n",
    "    only_model_name = f'deberta-v3-{select}'\n",
    "    batch_size = 8\n",
    "    fold = 4\n",
    "    seed = 42\n",
    "    max_len = 512\n",
    "    lr = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6e84a74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T09:04:26.992478Z",
     "iopub.status.busy": "2023-10-11T09:04:26.991919Z",
     "iopub.status.idle": "2023-10-11T09:12:52.544458Z",
     "shell.execute_reply": "2023-10-11T09:12:52.543446Z"
    },
    "papermill": {
     "duration": 505.561854,
     "end_time": "2023-10-11T09:12:52.546678",
     "exception": false,
     "start_time": "2023-10-11T09:04:26.984824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompts_train = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/prompts_train.csv')\n",
    "summaries_train = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/summaries_train.csv')\n",
    "prompts_test = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/prompts_test.csv')\n",
    "summaries_test = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/summaries_test.csv')\n",
    "\n",
    "spell = SpellChecker()\n",
    "def summaries_spell_check(df):\n",
    "    total_mis_tokens = []\n",
    "    for text in df['text']:\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        mis_tokens = [token for token in spell.unknown(tokens) if token.isalpha()]\n",
    "        total_mis_tokens.append(mis_tokens)\n",
    "    df['mis_tokens'] = total_mis_tokens\n",
    "    return df\n",
    "\n",
    "summaries_train = summaries_spell_check(summaries_train)\n",
    "summaries_test = summaries_spell_check(summaries_test)\n",
    "summaries_train['counts_mis_token'] = summaries_train['mis_tokens'].apply(lambda x : len(set(x)))\n",
    "summaries_test['counts_mis_token'] = summaries_test['mis_tokens'].apply(lambda x : len(set(x)))\n",
    "\n",
    "speller = Speller(lang='en')\n",
    "summaries_train[\"text\"] = summaries_train[\"text\"].apply(\n",
    "    lambda x: speller(x)\n",
    ")\n",
    "\n",
    "speller = Speller(lang='en')\n",
    "summaries_test[\"text\"] = summaries_test[\"text\"].apply(\n",
    "    lambda x: speller(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5a9eed3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T09:12:52.561705Z",
     "iopub.status.busy": "2023-10-11T09:12:52.561430Z",
     "iopub.status.idle": "2023-10-11T09:13:01.354321Z",
     "shell.execute_reply": "2023-10-11T09:13:01.353218Z"
    },
    "papermill": {
     "duration": 8.803134,
     "end_time": "2023-10-11T09:13:01.356748",
     "exception": false,
     "start_time": "2023-10-11T09:12:52.553614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "summaries_train['smog_index'] = summaries_train['text'].apply(lambda x : textstat.smog_index(x))\n",
    "summaries_test['smog_index'] = summaries_test['text'].apply(lambda x : textstat.smog_index(x))\n",
    "\n",
    "prompts_train['prompt_text_lower'] = prompts_train['prompt_text'].apply(lambda x : x.lower())\n",
    "prompts_test['prompt_text_lower'] = prompts_test['prompt_text'].apply(lambda x : x.lower())\n",
    "summaries_train['text_lower'] = summaries_train['text'].apply(lambda x : x.lower())\n",
    "summaries_test['text_lower'] = summaries_test['text'].apply(lambda x : x.lower())\n",
    "summaries_train['word_counts'] = summaries_train['text_lower'].apply(lambda x : len(set(x.split(' '))))\n",
    "summaries_test['word_counts'] = summaries_test['text_lower'].apply(lambda x : len(set(x.split(' '))))\n",
    "\n",
    "prompts_train['prompt_text_lower'] = prompts_train['prompt_text_lower'].str.replace('\\r','')\n",
    "prompts_train['prompt_text_lower'] = prompts_train['prompt_text_lower'].str.replace('\\n','')\n",
    "summaries_train['sentence_counts'] = summaries_train['text_lower'].apply(lambda x : len(set(x.split('.'))))\n",
    "summaries_test['sentence_counts'] = summaries_test['text_lower'].apply(lambda x : len(set(x.split('.'))))\n",
    "summaries_train[\"syntax_count\"] = summaries_train['text_lower'].apply(lambda x: x.count(\",\") + x.count(\"-\") + x.count(\";\") + x.count(\":\"))\n",
    "summaries_test[\"syntax_count\"] = summaries_test['text_lower'].apply(lambda x: x.count(\",\") + x.count(\"-\") + x.count(\";\") + x.count(\":\"))\n",
    "text_no_stopwords = []\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "stop_words.update(['.',',','”','“',';',':','?',\"''\",\"``\"])\n",
    "def prompt_no_stopwords(df):\n",
    "    text_no_stopwords = []\n",
    "    for text in df['prompt_text_lower']:\n",
    "        text_no_stopword = [token for token in nltk.word_tokenize(text) if token not in stop_words]\n",
    "        text_no_stopwords.append(text_no_stopword)\n",
    "    df['prompt_no_stop_words'] = text_no_stopwords\n",
    "    return df\n",
    "def summaries_no_stopwords(df):\n",
    "    text_no_stopwords = []\n",
    "    for text in df['text_lower']:\n",
    "        text_no_stopword = [token for token in nltk.word_tokenize(text) if token not in stop_words]\n",
    "        text_no_stopwords.append(text_no_stopword)\n",
    "    df['summaries_no_stop_words'] = text_no_stopwords\n",
    "    return df\n",
    "prompts_train = prompt_no_stopwords(prompts_train)\n",
    "prompts_test = prompt_no_stopwords(prompts_test)\n",
    "summaries_train = summaries_no_stopwords(summaries_train)\n",
    "summaries_test = summaries_no_stopwords(summaries_test)\n",
    "summaries_train['word_counts_with_stopword'] = summaries_train['summaries_no_stop_words'].apply(lambda x : len(set(x)))\n",
    "summaries_test['word_counts_with_stopword'] = summaries_test['summaries_no_stop_words'].apply(lambda x : len(set(x)))\n",
    "\n",
    "counts_duplicate = []\n",
    "own_unique_words = []\n",
    "\n",
    "for i in range(len(summaries_train)):\n",
    "    student_text = summaries_train.iloc[i]['summaries_no_stop_words']\n",
    "    prompt_id = summaries_train.iloc[i]['prompt_id']\n",
    "    splited_prompt = prompts_train[prompts_train['prompt_id'] == prompt_id]['prompt_no_stop_words'].tolist()[0]\n",
    "    count_duplicate = len(set(student_text) & set(splited_prompt))\n",
    "    counts_duplicate.append(count_duplicate)\n",
    "    own_unique_words.append(len(set(student_text) - set(splited_prompt)))\n",
    "summaries_train['counts_duplicate'] = counts_duplicate\n",
    "summaries_train['own_unique_words'] = own_unique_words\n",
    "counts_duplicate = []\n",
    "own_unique_words = []\n",
    "\n",
    "for i in range(len(summaries_test)):\n",
    "    student_text = summaries_test.iloc[i]['summaries_no_stop_words']\n",
    "    prompt_id = summaries_test.iloc[i]['prompt_id']\n",
    "    splited_prompt = prompts_test[prompts_test['prompt_id'] == prompt_id]['prompt_no_stop_words'].tolist()[0]\n",
    "    count_duplicate = len(set(student_text) & set(splited_prompt))\n",
    "    counts_duplicate.append(count_duplicate)\n",
    "    own_unique_words.append(len(set(student_text) - set(splited_prompt)))\n",
    "summaries_test['counts_duplicate'] = counts_duplicate\n",
    "summaries_test['own_unique_words'] = own_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5e0fef4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T09:13:01.373530Z",
     "iopub.status.busy": "2023-10-11T09:13:01.373227Z",
     "iopub.status.idle": "2023-10-11T09:13:03.180454Z",
     "shell.execute_reply": "2023-10-11T09:13:03.179356Z"
    },
    "papermill": {
     "duration": 1.818027,
     "end_time": "2023-10-11T09:13:03.182532",
     "exception": false,
     "start_time": "2023-10-11T09:13:01.364505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self, \n",
    "                model_name: str,\n",
    "                ) -> None:\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(f\"{model_name}\")\n",
    "        self.STOP_WORDS = set(nltk.corpus.stopwords.words('english'))\n",
    "        \n",
    "        self.spacy_ner_model = spacy.load('en_core_web_sm',)\n",
    "        self.speller = SpellChecker() #Speller(lang='en')\n",
    "        \n",
    "    def count_text_length(self, df: pd.DataFrame, col:str) -> pd.Series:\n",
    "        \"\"\" text length \"\"\"\n",
    "        tokenizer=self.tokenizer\n",
    "        return df[col].progress_apply(lambda x: len(tokenizer.encode(x)))\n",
    "\n",
    "    def word_overlap_count(self, row):\n",
    "        \"\"\" intersection(prompt_text, text) \"\"\"        \n",
    "        def check_is_stop_word(word):\n",
    "            return word in self.STOP_WORDS\n",
    "        \n",
    "        prompt_words = row['prompt_tokens']\n",
    "        summary_words = row['summary_tokens']\n",
    "        if self.STOP_WORDS:\n",
    "            prompt_words = list(filter(check_is_stop_word, prompt_words))\n",
    "            summary_words = list(filter(check_is_stop_word, summary_words))\n",
    "        return len(set(prompt_words).intersection(set(summary_words)))\n",
    "            \n",
    "    def ngrams(self, token, n):\n",
    "        ngrams = zip(*[token[i:] for i in range(n)])\n",
    "        return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "    def ngram_co_occurrence(self, row, n: int):\n",
    "        original_tokens = row['prompt_tokens']\n",
    "        summary_tokens = row['summary_tokens']\n",
    "\n",
    "        original_ngrams = set(self.ngrams(original_tokens, n))\n",
    "        summary_ngrams = set(self.ngrams(summary_tokens, n))\n",
    "\n",
    "        common_ngrams = original_ngrams.intersection(summary_ngrams)\n",
    "\n",
    "        return len(common_ngrams)\n",
    "    \n",
    "    def ner_overlap_count(self, row, mode:str):\n",
    "        model = self.spacy_ner_model\n",
    "        def clean_ners(ner_list):\n",
    "            return set([(ner[0].lower(), ner[1]) for ner in ner_list])\n",
    "        prompt = model(row['prompt_text'])\n",
    "        summary = model(row['text'])\n",
    "\n",
    "        if \"spacy\" in str(model):\n",
    "            prompt_ner = set([(token.text, token.label_) for token in prompt.ents])\n",
    "            summary_ner = set([(token.text, token.label_) for token in summary.ents])\n",
    "        elif \"stanza\" in str(model):\n",
    "            prompt_ner = set([(token.text, token.type) for token in prompt.ents])\n",
    "            summary_ner = set([(token.text, token.type) for token in summary.ents])\n",
    "        else:\n",
    "            raise Exception(\"Model not supported\")\n",
    "\n",
    "        prompt_ner = clean_ners(prompt_ner)\n",
    "        summary_ner = clean_ners(summary_ner)\n",
    "\n",
    "        intersecting_ners = prompt_ner.intersection(summary_ner)\n",
    "        \n",
    "        ner_dict = dict(Counter([ner[1] for ner in intersecting_ners]))\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            return ner_dict\n",
    "        elif mode == \"test\":\n",
    "            return {key: ner_dict.get(key) for key in self.ner_keys}\n",
    "\n",
    "    \n",
    "    def quotes_count(self, row):\n",
    "        summary = row['text']\n",
    "        text = row['prompt_text']\n",
    "        quotes_from_summary = re.findall(r'\"([^\"]*)\"', summary)\n",
    "        if len(quotes_from_summary)>0:\n",
    "            return [quote in text for quote in quotes_from_summary].count(True)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def spelling(self, text):\n",
    "        \n",
    "        wordlist=text.split()\n",
    "        amount_miss = len(list(self.speller.unknown(wordlist)))\n",
    "\n",
    "        return amount_miss\n",
    "    \n",
    "    def run(self, \n",
    "            prompts: pd.DataFrame,\n",
    "            summaries:pd.DataFrame,\n",
    "            mode:str\n",
    "        ) -> pd.DataFrame:\n",
    "        \n",
    "        # before merge preprocess\n",
    "        prompts[\"prompt_length\"] = prompts[\"prompt_text\"].apply(\n",
    "            lambda x: len(self.tokenizer.encode(x))\n",
    "        )\n",
    "        prompts[\"prompt_tokens\"] = prompts[\"prompt_text\"].apply(\n",
    "            lambda x: self.tokenizer.convert_ids_to_tokens(\n",
    "                self.tokenizer.encode(x), \n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "        summaries[\"summary_length\"] = summaries[\"text\"].apply(\n",
    "            lambda x: len(self.tokenizer.encode(x))\n",
    "        )\n",
    "        summaries[\"summary_tokens\"] = summaries[\"text\"].apply(\n",
    "            lambda x: self.tokenizer.convert_ids_to_tokens(\n",
    "                self.tokenizer.encode(x), \n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "        )\n",
    "        summaries[\"splling_err_num\"] = summaries[\"text\"].apply(self.spelling)\n",
    "\n",
    "        input_df = summaries.merge(prompts, how=\"left\", on=\"prompt_id\")\n",
    "\n",
    "        input_df['length_ratio'] = input_df['summary_length'] / input_df['prompt_length']\n",
    "        \n",
    "        input_df['word_overlap_count'] = input_df.apply(self.word_overlap_count, axis=1)\n",
    "        input_df['bigram_overlap_count'] = input_df.apply(\n",
    "            self.ngram_co_occurrence,args=(2,), axis=1 \n",
    "        )\n",
    "        input_df['trigram_overlap_count'] = input_df.apply(\n",
    "            self.ngram_co_occurrence, args=(3,), axis=1\n",
    "        )\n",
    "        \n",
    "        input_df['quotes_count'] = input_df.apply(self.quotes_count, axis=1)\n",
    "        \n",
    "        return input_df.drop(columns=[\"summary_tokens\", \"prompt_tokens\"])\n",
    "    \n",
    "preprocessor = Preprocessor(model_name=cfg.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3af08783",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T09:13:03.197184Z",
     "iopub.status.busy": "2023-10-11T09:13:03.196917Z",
     "iopub.status.idle": "2023-10-11T09:13:17.175551Z",
     "shell.execute_reply": "2023-10-11T09:13:17.174593Z"
    },
    "papermill": {
     "duration": 13.988168,
     "end_time": "2023-10-11T09:13:17.177688",
     "exception": false,
     "start_time": "2023-10-11T09:13:03.189520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = preprocessor.run(prompts_train, summaries_train, mode=\"train\")\n",
    "test = preprocessor.run(prompts_test, summaries_test, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a22eadc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T09:13:17.192691Z",
     "iopub.status.busy": "2023-10-11T09:13:17.192420Z",
     "iopub.status.idle": "2023-10-11T09:13:17.201352Z",
     "shell.execute_reply": "2023-10-11T09:13:17.200547Z"
    },
    "papermill": {
     "duration": 0.018185,
     "end_time": "2023-10-11T09:13:17.203025",
     "exception": false,
     "start_time": "2023-10-11T09:13:17.184840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train = summaries_train.merge(prompts_train, how='left', on=\"prompt_id\")\n",
    "# test = summaries_test.merge(prompts_test, how='left', on=\"prompt_id\")\n",
    "\n",
    "# train['fold'] = -1\n",
    "# fold = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "# for n, (train_index, val_index) in enumerate(fold.split(train, train['prompt_id'])):\n",
    "#     train.loc[val_index, 'fold'] = n\n",
    "# train['fold'] = train['fold'].astype(int)\n",
    "\n",
    "prompt_dict = {id : index for index,id in enumerate(train['prompt_id'].unique())}\n",
    "train['fold'] = -1\n",
    "train['fold'] = train['prompt_id'].apply(lambda x : prompt_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acc19bde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T09:13:17.217004Z",
     "iopub.status.busy": "2023-10-11T09:13:17.216754Z",
     "iopub.status.idle": "2023-10-11T09:13:18.075202Z",
     "shell.execute_reply": "2023-10-11T09:13:18.074252Z"
    },
    "papermill": {
     "duration": 0.867692,
     "end_time": "2023-10-11T09:13:18.077139",
     "exception": false,
     "start_time": "2023-10-11T09:13:17.209447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n",
    "cfg.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e9dd78d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T09:13:18.092545Z",
     "iopub.status.busy": "2023-10-11T09:13:18.092253Z",
     "iopub.status.idle": "2023-10-11T09:13:18.099800Z",
     "shell.execute_reply": "2023-10-11T09:13:18.098896Z"
    },
    "papermill": {
     "duration": 0.017265,
     "end_time": "2023-10-11T09:13:18.101505",
     "exception": false,
     "start_time": "2023-10-11T09:13:18.084240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.tokenizer = cfg.tokenizer\n",
    "        self.max_len = cfg.max_len\n",
    "        self.fp = df['prompt_text'].values\n",
    "        self.pq = df['prompt_question'].values\n",
    "        self.title = df['prompt_title'].values\n",
    "        self.text = df['text'].values\n",
    "        self.targets = df[['content','wording']].values\n",
    "        self.id = df['student_id'].values\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self , index):\n",
    "        pq   =   self.pq[index]\n",
    "        title = self.title[index]\n",
    "        text =   self.text[index]\n",
    "        fp = self.fp[index]\n",
    "        full_text = 'System Message: You are an AI assistant that scores summarized text based on provided questions and text summaries.'+ self.tokenizer.sep_token + pq + self.tokenizer.sep_token + text\n",
    "#        full_text = f'Question : {pq}' + self.tokenizer.sep_token + f'Summary : {text}'\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "                        full_text,\n",
    "                        truncation=True,\n",
    "                        add_special_tokens=True,\n",
    "                        max_length=self.max_len,\n",
    "                        padding='max_length'\n",
    "                    )\n",
    "        \n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        target = self.targets[index]\n",
    "        \n",
    "   \n",
    "        return {\n",
    "            'input_ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(mask, dtype=torch.long),\n",
    "            \n",
    "        } , torch.tensor(target, dtype=torch.float), self.id[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70b7e5a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T09:13:18.115795Z",
     "iopub.status.busy": "2023-10-11T09:13:18.115545Z",
     "iopub.status.idle": "2023-10-11T09:13:18.122329Z",
     "shell.execute_reply": "2023-10-11T09:13:18.121454Z"
    },
    "papermill": {
     "duration": 0.01593,
     "end_time": "2023-10-11T09:13:18.124047",
     "exception": false,
     "start_time": "2023-10-11T09:13:18.108117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.tokenizer = cfg.tokenizer\n",
    "        self.max_len = cfg.max_len\n",
    "        self.fp = df['prompt_text'].values\n",
    "        self.pq = df['prompt_question'].values\n",
    "        self.title = df['prompt_title'].values\n",
    "        self.text = df['text'].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self , index):\n",
    "        pq   =   self.pq[index]\n",
    "        title = self.title[index]\n",
    "        text =   self.text[index]\n",
    "        fp = self.fp[index]\n",
    "        full_text = 'System Message: You are an AI assistant that scores summarized text based on provided questions and text summaries.'+ self.tokenizer.sep_token + pq + self.tokenizer.sep_token + text\n",
    "#        full_text = f'Question : {pq}' + self.tokenizer.sep_token + f'Summary : {text}'\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "                        full_text,\n",
    "                        truncation=True,\n",
    "                        add_special_tokens=True,\n",
    "                        max_length=self.max_len,\n",
    "                        padding='max_length'\n",
    "                    )\n",
    "        \n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "   \n",
    "        return {\n",
    "            'input_ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(mask, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8241fa38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T09:13:18.138612Z",
     "iopub.status.busy": "2023-10-11T09:13:18.137903Z",
     "iopub.status.idle": "2023-10-11T09:13:18.144814Z",
     "shell.execute_reply": "2023-10-11T09:13:18.144043Z"
    },
    "papermill": {
     "duration": 0.015908,
     "end_time": "2023-10-11T09:13:18.146438",
     "exception": false,
     "start_time": "2023-10-11T09:13:18.130530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validation(model, val_loader):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    pred_list = []\n",
    "    label_list = []\n",
    "    id_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch, labels, s_id  in tqdm(iter(val_loader)):\n",
    "            batch = {i: v.to(\"cuda\") for i, v in batch.items()}\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            output = model(**batch,labels = labels)\n",
    "            pred = output.logits\n",
    "            loss = output.loss\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "            pred_list += pred.detach().cpu().tolist()\n",
    "            label_list += labels.detach().cpu().tolist()\n",
    "            id_list += s_id\n",
    "        _val_loss = np.mean(val_loss)\n",
    "        pred_list_re = np.array(pred_list).reshape(-1,2)[:,]\n",
    "        label_list_re = np.array(label_list).reshape(-1,2)[:,]\n",
    "        scores = compute_mcrmse(pred_list_re, label_list_re)\n",
    "\n",
    "    return _val_loss, scores, pred_list_re, label_list_re, id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15fd5cf4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T09:13:18.160836Z",
     "iopub.status.busy": "2023-10-11T09:13:18.160587Z",
     "iopub.status.idle": "2023-10-11T09:13:18.165730Z",
     "shell.execute_reply": "2023-10-11T09:13:18.164783Z"
    },
    "papermill": {
     "duration": 0.013944,
     "end_time": "2023-10-11T09:13:18.167284",
     "exception": false,
     "start_time": "2023-10-11T09:13:18.153340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(model, test_loader):\n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(iter(test_loader)):\n",
    "            batch = {i: v.to(\"cuda\") for i, v in batch.items()}\n",
    "            output = model(**batch)\n",
    "            pred = output.logits\n",
    "            \n",
    "            pred_list += pred.detach().cpu().tolist()\n",
    "        pred_list_re = np.array(pred_list).reshape(-1,2)[:,]\n",
    "\n",
    "    return pred_list_re,pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a7f2e7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T09:13:18.181666Z",
     "iopub.status.busy": "2023-10-11T09:13:18.181286Z",
     "iopub.status.idle": "2023-10-11T09:13:18.186063Z",
     "shell.execute_reply": "2023-10-11T09:13:18.185202Z"
    },
    "papermill": {
     "duration": 0.013602,
     "end_time": "2023-10-11T09:13:18.187677",
     "exception": false,
     "start_time": "2023-10-11T09:13:18.174075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_fold(df, n_fold):\n",
    "    dftrain = df[df['fold']!= n_fold]\n",
    "    dfvalid = df[df['fold']== n_fold]\n",
    "    \n",
    "    train_dataset = CustomDataset(dftrain)\n",
    "    valid_dataset = CustomDataset(dfvalid)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset , batch_size=cfg.batch_size, num_workers=0, shuffle=True, pin_memory=True) \n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset , batch_size=cfg.batch_size, num_workers=0, shuffle=False, pin_memory=True) \n",
    "    \n",
    "    return train_loader , valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c19434ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T09:13:18.201849Z",
     "iopub.status.busy": "2023-10-11T09:13:18.201251Z",
     "iopub.status.idle": "2023-10-11T09:13:18.206654Z",
     "shell.execute_reply": "2023-10-11T09:13:18.205763Z"
    },
    "papermill": {
     "duration": 0.014175,
     "end_time": "2023-10-11T09:13:18.208236",
     "exception": false,
     "start_time": "2023-10-11T09:13:18.194061",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    rmse = mean_squared_error(labels, predictions, squared=False)\n",
    "    return {\"rmse\": rmse}\n",
    "\n",
    "def compute_mcrmse(preds, labels):\n",
    "    col_rmse = np.sqrt(np.mean((preds - labels) ** 2, axis=0))\n",
    "    mcrmse = np.mean(col_rmse)\n",
    "\n",
    "    return {\n",
    "        \"content_rmse\": col_rmse[0],\n",
    "        \"wording_rmse\": col_rmse[1],\n",
    "        \"mcrmse\": mcrmse,\n",
    "    }\n",
    "\n",
    "def compt_score(content_true, content_pred, wording_true, wording_pred):\n",
    "    content_score = mean_squared_error(content_true, content_pred)**(1/2)\n",
    "    wording_score = mean_squared_error(wording_true, wording_pred)**(1/2)\n",
    "    \n",
    "    return (content_score + wording_score)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6ccdbaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T09:13:18.222533Z",
     "iopub.status.busy": "2023-10-11T09:13:18.221761Z",
     "iopub.status.idle": "2023-10-11T09:23:52.007335Z",
     "shell.execute_reply": "2023-10-11T09:23:52.006403Z"
    },
    "papermill": {
     "duration": 633.794509,
     "end_time": "2023-10-11T09:23:52.009115",
     "exception": false,
     "start_time": "2023-10-11T09:13:18.214606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /kaggle/input/deberta-v3-large-hf-weights were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/deberta-v3-large-hf-weights and are newly initialized: ['pooler.dense.bias', 'classifier.weight', 'pooler.dense.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,375,684 || all params: 436,508,676 || trainable%: 0.5442466852594701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138/138 [01:33<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content_rmse': 0.45821782338869066, 'wording_rmse': 0.6380222629499115, 'mcrmse': 0.5481200431693011}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /kaggle/input/deberta-v3-large-hf-weights were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/deberta-v3-large-hf-weights and are newly initialized: ['pooler.dense.bias', 'classifier.weight', 'pooler.dense.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,375,684 || all params: 436,508,676 || trainable%: 0.5442466852594701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [02:45<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content_rmse': 0.43231728230633665, 'wording_rmse': 0.5337774938795848, 'mcrmse': 0.48304738809296077}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /kaggle/input/deberta-v3-large-hf-weights were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/deberta-v3-large-hf-weights and are newly initialized: ['pooler.dense.bias', 'classifier.weight', 'pooler.dense.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,375,684 || all params: 436,508,676 || trainable%: 0.5442466852594701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 252/252 [02:46<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content_rmse': 0.5398186384714923, 'wording_rmse': 0.7936646148285021, 'mcrmse': 0.6667416266499973}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /kaggle/input/deberta-v3-large-hf-weights were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/deberta-v3-large-hf-weights and are newly initialized: ['pooler.dense.bias', 'classifier.weight', 'pooler.dense.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,375,684 || all params: 436,508,676 || trainable%: 0.5442466852594701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 258/258 [02:50<00:00,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content_rmse': 0.5565992141647131, 'wording_rmse': 0.5382900608530238, 'mcrmse': 0.5474446375088684}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_pred = []\n",
    "total_id = []\n",
    "\n",
    "for n_fold in range(cfg.fold):\n",
    "    model_config = AutoConfig.from_pretrained(cfg.model_name)\n",
    "    model_config.update({\n",
    "            \"num_labels\": 2,\n",
    "            \"problem_type\": \"regression\"\n",
    "        })\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(cfg.model_name,config = model_config)\n",
    "    lora_config = LoraConfig.from_pretrained(f'/kaggle/input/deberta-v3-large-adapter-new/fold_{n_fold}')\n",
    "    peft_model = PeftModel.from_pretrained(model,f'/kaggle/input/deberta-v3-large-adapter-new/fold_{n_fold}')\n",
    "    peft_model.print_trainable_parameters()\n",
    "    peft_model = peft_model.merge_and_unload()\n",
    "    peft_model = peft_model.to('cuda')\n",
    "    train_loader , valid_loader = prepare_fold(train, n_fold)\n",
    "    _val_loss, scores, valid_pred, valid_label, id_list = validation(model, valid_loader)\n",
    "    print(scores)\n",
    "    total_pred += valid_pred.tolist()\n",
    "    total_id += id_list\n",
    "    del model, peft_model, model_config, lora_config\n",
    "\n",
    "train_pred_df = pd.DataFrame(data = zip(total_id, np.array(total_pred)[:,0].tolist(), np.array(total_pred)[:,1].tolist()) ,columns = ['student_id', 'content_pred', 'wording_pred'])\n",
    "train = train.merge(train_pred_df, how = 'left', on = 'student_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9326f2d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T09:23:52.097196Z",
     "iopub.status.busy": "2023-10-11T09:23:52.096732Z",
     "iopub.status.idle": "2023-10-11T09:23:52.101688Z",
     "shell.execute_reply": "2023-10-11T09:23:52.100765Z"
    },
    "papermill": {
     "duration": 0.050433,
     "end_time": "2023-10-11T09:23:52.103279",
     "exception": false,
     "start_time": "2023-10-11T09:23:52.052846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = TestDataset(test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset , batch_size=cfg.batch_size, num_workers=0, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdd74213",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T09:23:52.190742Z",
     "iopub.status.busy": "2023-10-11T09:23:52.190355Z",
     "iopub.status.idle": "2023-10-11T09:24:16.210071Z",
     "shell.execute_reply": "2023-10-11T09:24:16.208958Z"
    },
    "papermill": {
     "duration": 24.068875,
     "end_time": "2023-10-11T09:24:16.215072",
     "exception": false,
     "start_time": "2023-10-11T09:23:52.146197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** fold 0 ********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /kaggle/input/deberta-v3-large-hf-weights were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/deberta-v3-large-hf-weights and are newly initialized: ['pooler.dense.bias', 'classifier.weight', 'pooler.dense.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,375,684 || all params: 436,508,676 || trainable%: 0.5442466852594701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** fold 1 ********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /kaggle/input/deberta-v3-large-hf-weights were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/deberta-v3-large-hf-weights and are newly initialized: ['pooler.dense.bias', 'classifier.weight', 'pooler.dense.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,375,684 || all params: 436,508,676 || trainable%: 0.5442466852594701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** fold 2 ********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /kaggle/input/deberta-v3-large-hf-weights were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/deberta-v3-large-hf-weights and are newly initialized: ['pooler.dense.bias', 'classifier.weight', 'pooler.dense.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,375,684 || all params: 436,508,676 || trainable%: 0.5442466852594701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** fold 3 ********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /kaggle/input/deberta-v3-large-hf-weights were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/deberta-v3-large-hf-weights and are newly initialized: ['pooler.dense.bias', 'classifier.weight', 'pooler.dense.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,375,684 || all params: 436,508,676 || trainable%: 0.5442466852594701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.82it/s]\n"
     ]
    }
   ],
   "source": [
    "total_content_pred = np.zeros(shape = (len(test)))\n",
    "total_wording_pred = np.zeros(shape = (len(test)))\n",
    "for n_fold in range(cfg.fold):\n",
    "    print('******** fold' , n_fold , '********')\n",
    "    model_config = AutoConfig.from_pretrained(cfg.model_name)\n",
    "    model_config.update({\n",
    "            \"num_labels\": 2,\n",
    "            \"problem_type\": \"regression\"\n",
    "        })\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(cfg.model_name,config = model_config)\n",
    "    lora_config = LoraConfig.from_pretrained(f'/kaggle/input/deberta-v3-large-adapter-new/fold_{n_fold}')\n",
    "    peft_model = PeftModel.from_pretrained(model,f'/kaggle/input/deberta-v3-large-adapter-new/fold_{n_fold}')\n",
    "    peft_model.print_trainable_parameters()\n",
    "    peft_model = peft_model.merge_and_unload()\n",
    "    peft_model = peft_model.to('cuda')\n",
    "    pred_list_re,pred_list = predict(model, test_loader)\n",
    "    total_content_pred += pred_list_re[:, 0]\n",
    "    total_wording_pred += pred_list_re[:, 1]\n",
    "\n",
    "    del model, model_config, peft_model, lora_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2033219c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T09:24:16.308009Z",
     "iopub.status.busy": "2023-10-11T09:24:16.307102Z",
     "iopub.status.idle": "2023-10-11T09:24:16.311432Z",
     "shell.execute_reply": "2023-10-11T09:24:16.310714Z"
    },
    "papermill": {
     "duration": 0.052371,
     "end_time": "2023-10-11T09:24:16.313108",
     "exception": false,
     "start_time": "2023-10-11T09:24:16.260737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pred,pred_list = predict(peft_model,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d4cd9c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T09:24:16.405096Z",
     "iopub.status.busy": "2023-10-11T09:24:16.404236Z",
     "iopub.status.idle": "2023-10-11T09:24:16.409624Z",
     "shell.execute_reply": "2023-10-11T09:24:16.408840Z"
    },
    "papermill": {
     "duration": 0.053902,
     "end_time": "2023-10-11T09:24:16.411244",
     "exception": false,
     "start_time": "2023-10-11T09:24:16.357342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test['content_pred'] = total_content_pred/4\n",
    "test['wording_pred'] = total_wording_pred/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd8b5535",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T09:24:16.503195Z",
     "iopub.status.busy": "2023-10-11T09:24:16.502664Z",
     "iopub.status.idle": "2023-10-11T09:24:16.508979Z",
     "shell.execute_reply": "2023-10-11T09:24:16.508120Z"
    },
    "papermill": {
     "duration": 0.053716,
     "end_time": "2023-10-11T09:24:16.510691",
     "exception": false,
     "start_time": "2023-10-11T09:24:16.456975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['student_id', 'prompt_id', 'text', 'mis_tokens', 'counts_mis_token',\n",
       "       'smog_index', 'text_lower', 'word_counts', 'sentence_counts',\n",
       "       'syntax_count', 'summaries_no_stop_words', 'word_counts_with_stopword',\n",
       "       'counts_duplicate', 'own_unique_words', 'summary_length',\n",
       "       'splling_err_num', 'prompt_question', 'prompt_title', 'prompt_text',\n",
       "       'prompt_text_lower', 'prompt_no_stop_words', 'prompt_length',\n",
       "       'length_ratio', 'word_overlap_count', 'bigram_overlap_count',\n",
       "       'trigram_overlap_count', 'quotes_count', 'content_pred',\n",
       "       'wording_pred'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67569508",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T09:24:16.602109Z",
     "iopub.status.busy": "2023-10-11T09:24:16.601805Z",
     "iopub.status.idle": "2023-10-11T09:24:16.608115Z",
     "shell.execute_reply": "2023-10-11T09:24:16.607221Z"
    },
    "papermill": {
     "duration": 0.054796,
     "end_time": "2023-10-11T09:24:16.609802",
     "exception": false,
     "start_time": "2023-10-11T09:24:16.555006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "all_columns = ['length_ratio', 'word_overlap_count', 'bigram_overlap_count', 'trigram_overlap_count', 'quotes_count','smog_index', 'word_counts', 'sentence_counts', 'syntax_count', 'word_counts_with_stopword', 'counts_mis_token', 'counts_duplicate'] # , 'content_pred', 'wording_pred'\n",
    "column_list = []\n",
    "for n in range(3, len(all_columns) + 1):\n",
    "    for column in combinations(all_columns, n):\n",
    "        column_list.append(list(column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e60b83b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T09:24:16.703571Z",
     "iopub.status.busy": "2023-10-11T09:24:16.703272Z",
     "iopub.status.idle": "2023-10-11T09:29:56.005844Z",
     "shell.execute_reply": "2023-10-11T09:29:56.004875Z"
    },
    "papermill": {
     "duration": 339.354152,
     "end_time": "2023-10-11T09:29:56.008378",
     "exception": false,
     "start_time": "2023-10-11T09:24:16.654226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4017/4017 [05:39<00:00, 11.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45750915841065204 ['length_ratio', 'word_overlap_count', 'bigram_overlap_count', 'trigram_overlap_count', 'quotes_count', 'smog_index', 'word_counts', 'sentence_counts', 'word_counts_with_stopword', 'counts_mis_token', 'counts_duplicate']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# lgbm - content\n",
    "best_score = 1\n",
    "best_column = []\n",
    "for column in tqdm(column_list):\n",
    "    X = train[column]\n",
    "    y = train['content']\n",
    "    test_x = test[column]\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size = 0.2, random_state=42, shuffle= True)\n",
    "    \n",
    "    model = lightgbm.LGBMRegressor(random_state=42, max_depth=5,learning_rate=0.05)\n",
    "    model.fit(train_x,train_y, eval_set = (valid_x, valid_y), eval_metric = 'rmse', verbose=-1)\n",
    "\n",
    "    score = model.best_score_['valid_0']['rmse']\n",
    "    if best_score > score:\n",
    "        best_score = score\n",
    "        best_column = column\n",
    "        lgbm_content_pred = model.predict(test_x)\n",
    "print(best_score, best_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe3b29cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T09:29:56.274469Z",
     "iopub.status.busy": "2023-10-11T09:29:56.274118Z",
     "iopub.status.idle": "2023-10-11T09:35:29.680373Z",
     "shell.execute_reply": "2023-10-11T09:35:29.679123Z"
    },
    "papermill": {
     "duration": 333.541608,
     "end_time": "2023-10-11T09:35:29.682917",
     "exception": false,
     "start_time": "2023-10-11T09:29:56.141309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4017/4017 [05:33<00:00, 12.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5854623517027437 ['length_ratio', 'trigram_overlap_count', 'smog_index', 'word_counts', 'sentence_counts', 'syntax_count', 'counts_mis_token', 'counts_duplicate']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# lgbm - wording\n",
    "best_score = 1\n",
    "best_column = []\n",
    "for column in tqdm(column_list):\n",
    "    X = train[column]\n",
    "    y = train['wording']\n",
    "    test_x = test[column]\n",
    "\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size = 0.2, random_state=42, shuffle= True)\n",
    "    \n",
    "    model = lightgbm.LGBMRegressor(random_state=42, max_depth=5,learning_rate=0.05)\n",
    "    model.fit(train_x,train_y, eval_set = (valid_x, valid_y), eval_metric = 'rmse', verbose=-1)\n",
    "\n",
    "    score = model.best_score_['valid_0']['rmse']\n",
    "    if best_score > score:\n",
    "        best_score = score\n",
    "        best_column = column\n",
    "        lgbm_wording_pred = model.predict(test_x)\n",
    "print(best_score, best_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ba2374c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T09:35:30.118441Z",
     "iopub.status.busy": "2023-10-11T09:35:30.118103Z",
     "iopub.status.idle": "2023-10-11T09:35:30.122597Z",
     "shell.execute_reply": "2023-10-11T09:35:30.121623Z"
    },
    "papermill": {
     "duration": 0.223996,
     "end_time": "2023-10-11T09:35:30.124233",
     "exception": false,
     "start_time": "2023-10-11T09:35:29.900237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # cat - content\n",
    "# best_score = 1\n",
    "# best_column = []\n",
    "# for column in tqdm(column_list):\n",
    "#     X = train[column]\n",
    "#     y = train['content']\n",
    "#     test_x = test[column]\n",
    "#     train_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size = 0.2, random_state=42, shuffle= True)\n",
    "    \n",
    "#     model = catboost.CatBoostRegressor(random_state=42, max_depth=5,learning_rate=0.05,objective = 'RMSE',verbose=False)\n",
    "#     model.fit(train_x,train_y,eval_set = (valid_x,valid_y))\n",
    "\n",
    "#     score = model.best_score_['validation']['RMSE']\n",
    "#     if best_score > score:\n",
    "#         best_score = score\n",
    "#         best_column = column\n",
    "#         cat_content_pred = model.predict(test_x)\n",
    "# print(best_score, best_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e33f4452",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T09:35:30.572267Z",
     "iopub.status.busy": "2023-10-11T09:35:30.571947Z",
     "iopub.status.idle": "2023-10-11T11:03:02.911037Z",
     "shell.execute_reply": "2023-10-11T11:03:02.909800Z"
    },
    "papermill": {
     "duration": 5252.564748,
     "end_time": "2023-10-11T11:03:02.913286",
     "exception": false,
     "start_time": "2023-10-11T09:35:30.348538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4017/4017 [1:27:32<00:00,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5780044619168215 ['length_ratio', 'word_overlap_count', 'trigram_overlap_count', 'smog_index', 'word_counts', 'sentence_counts', 'word_counts_with_stopword', 'counts_mis_token', 'counts_duplicate']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# cat - wording\n",
    "best_score = 1\n",
    "best_column = []\n",
    "for column in tqdm(column_list):\n",
    "    X = train[column]\n",
    "    y = train['wording']\n",
    "    test_x = test[column]\n",
    "\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size = 0.2, random_state=42, shuffle= True)\n",
    "    \n",
    "    model = catboost.CatBoostRegressor(random_state=42, max_depth=5,learning_rate=0.05,objective = 'RMSE',verbose=False)\n",
    "    model.fit(train_x,train_y,eval_set = (valid_x,valid_y))\n",
    "\n",
    "    score = model.best_score_['validation']['RMSE']\n",
    "    if best_score > score:\n",
    "        best_score = score\n",
    "        best_column = column\n",
    "        cat_wording_pred = model.predict(test_x)\n",
    "print(best_score, best_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "018b1c51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T11:03:03.801780Z",
     "iopub.status.busy": "2023-10-11T11:03:03.801071Z",
     "iopub.status.idle": "2023-10-11T11:03:04.655115Z",
     "shell.execute_reply": "2023-10-11T11:03:04.653969Z"
    },
    "papermill": {
     "duration": 1.257083,
     "end_time": "2023-10-11T11:03:04.656954",
     "exception": true,
     "start_time": "2023-10-11T11:03:03.399871",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/IPython/core/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">interactiveshell.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3508</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">run_code</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3505 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> async_:                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3506 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">await</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">eval</span>(code_obj, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.user_global_ns, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.user_ns)               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3507 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>3508 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>exec(code_obj, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.user_global_ns, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.user_ns)                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3509 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">finally</span>:                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3510 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Reset our crash handler in place</span>                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3511 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>sys.excepthook = old_excepthook                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1 </span>sample_submission = pd.read_csv(<span style=\"color: #808000; text-decoration-color: #808000\">\"/kaggle/input/commonlit-evaluate-student-summaries/samp</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2 sample_submission[<span style=\"color: #808000; text-decoration-color: #808000\">'content'</span>] = (cat_content_pred + lgbm_content_pred)/<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span>sample_submission[<span style=\"color: #808000; text-decoration-color: #808000\">'wording'</span>] = (cat_wording_pred + lgbm_wording_pred)/<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 # sample_submission['content'] = total_content_pred/4</span>                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5 # sample_submission['wording'] = total_wording_pred/4</span>                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'cat_content_pred'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/IPython/core/\u001b[0m\u001b[1;33minteractiveshell.py\u001b[0m:\u001b[94m3508\u001b[0m in \u001b[92mrun_code\u001b[0m        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3505 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m async_:                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3506 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mawait\u001b[0m \u001b[96meval\u001b[0m(code_obj, \u001b[96mself\u001b[0m.user_global_ns, \u001b[96mself\u001b[0m.user_ns)               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3507 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m3508 \u001b[2m│   │   │   │   │   \u001b[0mexec(code_obj, \u001b[96mself\u001b[0m.user_global_ns, \u001b[96mself\u001b[0m.user_ns)                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3509 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mfinally\u001b[0m:                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3510 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# Reset our crash handler in place\u001b[0m                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3511 \u001b[0m\u001b[2m│   │   │   │   \u001b[0msys.excepthook = old_excepthook                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m2\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1 \u001b[0msample_submission = pd.read_csv(\u001b[33m\"\u001b[0m\u001b[33m/kaggle/input/commonlit-evaluate-student-summaries/samp\u001b[0m     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2 sample_submission[\u001b[33m'\u001b[0m\u001b[33mcontent\u001b[0m\u001b[33m'\u001b[0m] = (cat_content_pred + lgbm_content_pred)/\u001b[94m2\u001b[0m                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3 \u001b[0msample_submission[\u001b[33m'\u001b[0m\u001b[33mwording\u001b[0m\u001b[33m'\u001b[0m] = (cat_wording_pred + lgbm_wording_pred)/\u001b[94m2\u001b[0m                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m4 \u001b[0m\u001b[2m# sample_submission['content'] = total_content_pred/4\u001b[0m                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m5 \u001b[0m\u001b[2m# sample_submission['wording'] = total_wording_pred/4\u001b[0m                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'cat_content_pred'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_submission = pd.read_csv(\"/kaggle/input/commonlit-evaluate-student-summaries/sample_submission.csv\")\n",
    "sample_submission['content'] = (cat_content_pred + lgbm_content_pred)/2\n",
    "sample_submission['wording'] = (cat_wording_pred + lgbm_wording_pred)/2\n",
    "# sample_submission['content'] = total_content_pred/4\n",
    "# sample_submission['wording'] = total_wording_pred/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ae9f2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T07:29:29.806062Z",
     "iopub.status.busy": "2023-10-11T07:29:29.805382Z",
     "iopub.status.idle": "2023-10-11T07:29:29.814044Z",
     "shell.execute_reply": "2023-10-11T07:29:29.813219Z",
     "shell.execute_reply.started": "2023-10-11T07:29:29.806028Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7344.018933,
   "end_time": "2023-10-11T11:03:08.793025",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-11T09:00:44.774092",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
